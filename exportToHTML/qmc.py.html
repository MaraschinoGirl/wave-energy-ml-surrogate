<html>
<head>
<title>qmc.py</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<style type="text/css">
.s0 { color: #5f826b; font-style: italic;}
.s1 { color: #bcbec4;}
.s2 { color: #cf8e6d;}
.s3 { color: #bcbec4;}
.s4 { color: #7a7e85;}
.ln { color: #4b5059; font-weight: normal; font-style: normal; }
</style>
</head>
<body bgcolor="#1e1f22">
<table CELLSPACING=0 CELLPADDING=5 COLS=1 WIDTH="100%" BGCOLOR="#606060" >
<tr><td><center>
<font face="Arial, Helvetica" color="#000000">
qmc.py</font>
</center></td></tr></table>
<pre><a name="l1"><span class="ln">1    </span></a><span class="s0">r&quot;&quot;&quot; 
<a name="l2"><span class="ln">2    </span></a>==================================================== 
<a name="l3"><span class="ln">3    </span></a>Quasi-Monte Carlo submodule (:mod:`scipy.stats.qmc`) 
<a name="l4"><span class="ln">4    </span></a>==================================================== 
<a name="l5"><span class="ln">5    </span></a> 
<a name="l6"><span class="ln">6    </span></a>.. currentmodule:: scipy.stats.qmc 
<a name="l7"><span class="ln">7    </span></a> 
<a name="l8"><span class="ln">8    </span></a>This module provides Quasi-Monte Carlo generators and associated helper 
<a name="l9"><span class="ln">9    </span></a>functions. 
<a name="l10"><span class="ln">10   </span></a> 
<a name="l11"><span class="ln">11   </span></a> 
<a name="l12"><span class="ln">12   </span></a>Quasi-Monte Carlo 
<a name="l13"><span class="ln">13   </span></a>================= 
<a name="l14"><span class="ln">14   </span></a> 
<a name="l15"><span class="ln">15   </span></a>Engines 
<a name="l16"><span class="ln">16   </span></a>------- 
<a name="l17"><span class="ln">17   </span></a> 
<a name="l18"><span class="ln">18   </span></a>.. autosummary:: 
<a name="l19"><span class="ln">19   </span></a>   :toctree: generated/ 
<a name="l20"><span class="ln">20   </span></a> 
<a name="l21"><span class="ln">21   </span></a>   QMCEngine 
<a name="l22"><span class="ln">22   </span></a>   Sobol 
<a name="l23"><span class="ln">23   </span></a>   Halton 
<a name="l24"><span class="ln">24   </span></a>   LatinHypercube 
<a name="l25"><span class="ln">25   </span></a>   PoissonDisk 
<a name="l26"><span class="ln">26   </span></a>   MultinomialQMC 
<a name="l27"><span class="ln">27   </span></a>   MultivariateNormalQMC 
<a name="l28"><span class="ln">28   </span></a> 
<a name="l29"><span class="ln">29   </span></a>Helpers 
<a name="l30"><span class="ln">30   </span></a>------- 
<a name="l31"><span class="ln">31   </span></a> 
<a name="l32"><span class="ln">32   </span></a>.. autosummary:: 
<a name="l33"><span class="ln">33   </span></a>   :toctree: generated/ 
<a name="l34"><span class="ln">34   </span></a> 
<a name="l35"><span class="ln">35   </span></a>   discrepancy 
<a name="l36"><span class="ln">36   </span></a>   geometric_discrepancy 
<a name="l37"><span class="ln">37   </span></a>   update_discrepancy 
<a name="l38"><span class="ln">38   </span></a>   scale 
<a name="l39"><span class="ln">39   </span></a> 
<a name="l40"><span class="ln">40   </span></a> 
<a name="l41"><span class="ln">41   </span></a>Introduction to Quasi-Monte Carlo 
<a name="l42"><span class="ln">42   </span></a>================================= 
<a name="l43"><span class="ln">43   </span></a> 
<a name="l44"><span class="ln">44   </span></a>Quasi-Monte Carlo (QMC) methods [1]_, [2]_, [3]_ provide an 
<a name="l45"><span class="ln">45   </span></a>:math:`n \times d` array of numbers in :math:`[0,1]`. They can be used in 
<a name="l46"><span class="ln">46   </span></a>place of :math:`n` points from the :math:`U[0,1]^{d}` distribution. Compared to 
<a name="l47"><span class="ln">47   </span></a>random points, QMC points are designed to have fewer gaps and clumps. This is 
<a name="l48"><span class="ln">48   </span></a>quantified by discrepancy measures [4]_. From the Koksma-Hlawka 
<a name="l49"><span class="ln">49   </span></a>inequality [5]_ we know that low discrepancy reduces a bound on 
<a name="l50"><span class="ln">50   </span></a>integration error. Averaging a function :math:`f` over :math:`n` QMC points 
<a name="l51"><span class="ln">51   </span></a>can achieve an integration error close to :math:`O(n^{-1})` for well 
<a name="l52"><span class="ln">52   </span></a>behaved functions [2]_. 
<a name="l53"><span class="ln">53   </span></a> 
<a name="l54"><span class="ln">54   </span></a>Most QMC constructions are designed for special values of :math:`n` 
<a name="l55"><span class="ln">55   </span></a>such as powers of 2 or large primes. Changing the sample 
<a name="l56"><span class="ln">56   </span></a>size by even one can degrade their performance, even their 
<a name="l57"><span class="ln">57   </span></a>rate of convergence [6]_. For instance :math:`n=100` points may give less 
<a name="l58"><span class="ln">58   </span></a>accuracy than :math:`n=64` if the method was designed for :math:`n=2^m`. 
<a name="l59"><span class="ln">59   </span></a> 
<a name="l60"><span class="ln">60   </span></a>Some QMC constructions are extensible in :math:`n`: we can find 
<a name="l61"><span class="ln">61   </span></a>another special sample size :math:`n' &gt; n` and often an infinite 
<a name="l62"><span class="ln">62   </span></a>sequence of increasing special sample sizes. Some QMC 
<a name="l63"><span class="ln">63   </span></a>constructions are extensible in :math:`d`: we can increase the dimension, 
<a name="l64"><span class="ln">64   </span></a>possibly to some upper bound, and typically without requiring 
<a name="l65"><span class="ln">65   </span></a>special values of :math:`d`. Some QMC methods are extensible in 
<a name="l66"><span class="ln">66   </span></a>both :math:`n` and :math:`d`. 
<a name="l67"><span class="ln">67   </span></a> 
<a name="l68"><span class="ln">68   </span></a>QMC points are deterministic. That makes it hard to estimate the accuracy of 
<a name="l69"><span class="ln">69   </span></a>integrals estimated by averages over QMC points. Randomized QMC (RQMC) [7]_ 
<a name="l70"><span class="ln">70   </span></a>points are constructed so that each point is individually :math:`U[0,1]^{d}` 
<a name="l71"><span class="ln">71   </span></a>while collectively the :math:`n` points retain their low discrepancy. 
<a name="l72"><span class="ln">72   </span></a>One can make :math:`R` independent replications of RQMC points to 
<a name="l73"><span class="ln">73   </span></a>see how stable a computation is. From :math:`R` independent values, 
<a name="l74"><span class="ln">74   </span></a>a t-test (or bootstrap t-test [8]_) then gives approximate confidence 
<a name="l75"><span class="ln">75   </span></a>intervals on the mean value. Some RQMC methods produce a 
<a name="l76"><span class="ln">76   </span></a>root mean squared error that is actually :math:`o(1/n)` and smaller than 
<a name="l77"><span class="ln">77   </span></a>the rate seen in unrandomized QMC. An intuitive explanation is 
<a name="l78"><span class="ln">78   </span></a>that the error is a sum of many small ones and random errors 
<a name="l79"><span class="ln">79   </span></a>cancel in a way that deterministic ones do not. RQMC also 
<a name="l80"><span class="ln">80   </span></a>has advantages on integrands that are singular or, for other 
<a name="l81"><span class="ln">81   </span></a>reasons, fail to be Riemann integrable. 
<a name="l82"><span class="ln">82   </span></a> 
<a name="l83"><span class="ln">83   </span></a>(R)QMC cannot beat Bahkvalov's curse of dimension (see [9]_). For 
<a name="l84"><span class="ln">84   </span></a>any random or deterministic method, there are worst case functions 
<a name="l85"><span class="ln">85   </span></a>that will give it poor performance in high dimensions. A worst 
<a name="l86"><span class="ln">86   </span></a>case function for QMC might be 0 at all n points but very 
<a name="l87"><span class="ln">87   </span></a>large elsewhere. Worst case analyses get very pessimistic 
<a name="l88"><span class="ln">88   </span></a>in high dimensions. (R)QMC can bring a great improvement over 
<a name="l89"><span class="ln">89   </span></a>MC when the functions on which it is used are not worst case. 
<a name="l90"><span class="ln">90   </span></a>For instance (R)QMC can be especially effective on integrands 
<a name="l91"><span class="ln">91   </span></a>that are well approximated by sums of functions of 
<a name="l92"><span class="ln">92   </span></a>some small number of their input variables at a time [10]_, [11]_. 
<a name="l93"><span class="ln">93   </span></a>That property is often a surprising finding about those functions. 
<a name="l94"><span class="ln">94   </span></a> 
<a name="l95"><span class="ln">95   </span></a>Also, to see an improvement over IID MC, (R)QMC requires a bit of smoothness of 
<a name="l96"><span class="ln">96   </span></a>the integrand, roughly the mixed first order derivative in each direction, 
<a name="l97"><span class="ln">97   </span></a>:math:`\partial^d f/\partial x_1 \cdots \partial x_d`, must be integral. 
<a name="l98"><span class="ln">98   </span></a>For instance, a function that is 1 inside the hypersphere and 0 outside of it 
<a name="l99"><span class="ln">99   </span></a>has infinite variation in the sense of Hardy and Krause for any dimension 
<a name="l100"><span class="ln">100  </span></a>:math:`d = 2`. 
<a name="l101"><span class="ln">101  </span></a> 
<a name="l102"><span class="ln">102  </span></a>Scrambled nets are a kind of RQMC that have some valuable robustness 
<a name="l103"><span class="ln">103  </span></a>properties [12]_. If the integrand is square integrable, they give variance 
<a name="l104"><span class="ln">104  </span></a>:math:`var_{SNET} = o(1/n)`. There is a finite upper bound on 
<a name="l105"><span class="ln">105  </span></a>:math:`var_{SNET} / var_{MC}` that holds simultaneously for every square 
<a name="l106"><span class="ln">106  </span></a>integrable integrand. Scrambled nets satisfy a strong law of large numbers 
<a name="l107"><span class="ln">107  </span></a>for :math:`f` in :math:`L^p` when :math:`p&gt;1`. In some 
<a name="l108"><span class="ln">108  </span></a>special cases there is a central limit theorem [13]_. For smooth enough 
<a name="l109"><span class="ln">109  </span></a>integrands they can achieve RMSE nearly :math:`O(n^{-3})`. See [12]_ 
<a name="l110"><span class="ln">110  </span></a>for references about these properties. 
<a name="l111"><span class="ln">111  </span></a> 
<a name="l112"><span class="ln">112  </span></a>The main kinds of QMC methods are lattice rules [14]_ and digital 
<a name="l113"><span class="ln">113  </span></a>nets and sequences [2]_, [15]_. The theories meet up in polynomial 
<a name="l114"><span class="ln">114  </span></a>lattice rules [16]_ which can produce digital nets. Lattice rules 
<a name="l115"><span class="ln">115  </span></a>require some form of search for good constructions. For digital 
<a name="l116"><span class="ln">116  </span></a>nets there are widely used default constructions. 
<a name="l117"><span class="ln">117  </span></a> 
<a name="l118"><span class="ln">118  </span></a>The most widely used QMC methods are Sobol' sequences [17]_. 
<a name="l119"><span class="ln">119  </span></a>These are digital nets. They are extensible in both :math:`n` and :math:`d`. 
<a name="l120"><span class="ln">120  </span></a>They can be scrambled. The special sample sizes are powers 
<a name="l121"><span class="ln">121  </span></a>of 2. Another popular method are Halton sequences [18]_. 
<a name="l122"><span class="ln">122  </span></a>The constructions resemble those of digital nets. The earlier 
<a name="l123"><span class="ln">123  </span></a>dimensions have much better equidistribution properties than 
<a name="l124"><span class="ln">124  </span></a>later ones. There are essentially no special sample sizes. 
<a name="l125"><span class="ln">125  </span></a>They are not thought to be as accurate as Sobol' sequences. 
<a name="l126"><span class="ln">126  </span></a>They can be scrambled. The nets of Faure [19]_ are also widely 
<a name="l127"><span class="ln">127  </span></a>used. All dimensions are equally good, but the special sample 
<a name="l128"><span class="ln">128  </span></a>sizes grow rapidly with dimension :math:`d`. They can be scrambled. 
<a name="l129"><span class="ln">129  </span></a>The nets of Niederreiter and Xing [20]_ have the best asymptotic 
<a name="l130"><span class="ln">130  </span></a>properties but have not shown good empirical performance [21]_. 
<a name="l131"><span class="ln">131  </span></a> 
<a name="l132"><span class="ln">132  </span></a>Higher order digital nets are formed by a digit interleaving process 
<a name="l133"><span class="ln">133  </span></a>in the digits of the constructed points. They can achieve higher 
<a name="l134"><span class="ln">134  </span></a>levels of asymptotic accuracy given higher smoothness conditions on :math:`f` 
<a name="l135"><span class="ln">135  </span></a>and they can be scrambled [22]_. There is little or no empirical work 
<a name="l136"><span class="ln">136  </span></a>showing the improved rate to be attained. 
<a name="l137"><span class="ln">137  </span></a> 
<a name="l138"><span class="ln">138  </span></a>Using QMC is like using the entire period of a small random 
<a name="l139"><span class="ln">139  </span></a>number generator. The constructions are similar and so 
<a name="l140"><span class="ln">140  </span></a>therefore are the computational costs [23]_. 
<a name="l141"><span class="ln">141  </span></a> 
<a name="l142"><span class="ln">142  </span></a>(R)QMC is sometimes improved by passing the points through 
<a name="l143"><span class="ln">143  </span></a>a baker's transformation (tent function) prior to using them. 
<a name="l144"><span class="ln">144  </span></a>That function has the form :math:`1-2|x-1/2|`. As :math:`x` goes from 0 to 
<a name="l145"><span class="ln">145  </span></a>1, this function goes from 0 to 1 and then back. It is very 
<a name="l146"><span class="ln">146  </span></a>useful to produce a periodic function for lattice rules [14]_, 
<a name="l147"><span class="ln">147  </span></a>and sometimes it improves the convergence rate [24]_. 
<a name="l148"><span class="ln">148  </span></a> 
<a name="l149"><span class="ln">149  </span></a>It is not straightforward to apply QMC methods to Markov 
<a name="l150"><span class="ln">150  </span></a>chain Monte Carlo (MCMC).  We can think of MCMC as using 
<a name="l151"><span class="ln">151  </span></a>:math:`n=1` point in :math:`[0,1]^{d}` for very large :math:`d`, with 
<a name="l152"><span class="ln">152  </span></a>ergodic results corresponding to :math:`d \to \infty`. One proposal is 
<a name="l153"><span class="ln">153  </span></a>in [25]_ and under strong conditions an improved rate of convergence 
<a name="l154"><span class="ln">154  </span></a>has been shown [26]_. 
<a name="l155"><span class="ln">155  </span></a> 
<a name="l156"><span class="ln">156  </span></a>Returning to Sobol' points: there are many versions depending 
<a name="l157"><span class="ln">157  </span></a>on what are called direction numbers. Those are the result of 
<a name="l158"><span class="ln">158  </span></a>searches and are tabulated. A very widely used set of direction 
<a name="l159"><span class="ln">159  </span></a>numbers come from [27]_. It is extensible in dimension up to 
<a name="l160"><span class="ln">160  </span></a>:math:`d=21201`. 
<a name="l161"><span class="ln">161  </span></a> 
<a name="l162"><span class="ln">162  </span></a>References 
<a name="l163"><span class="ln">163  </span></a>---------- 
<a name="l164"><span class="ln">164  </span></a>.. [1] Owen, Art B. &quot;Monte Carlo Book: the Quasi-Monte Carlo parts.&quot; 2019. 
<a name="l165"><span class="ln">165  </span></a>.. [2] Niederreiter, Harald. &quot;Random number generation and quasi-Monte Carlo 
<a name="l166"><span class="ln">166  </span></a>   methods.&quot; Society for Industrial and Applied Mathematics, 1992. 
<a name="l167"><span class="ln">167  </span></a>.. [3] Dick, Josef, Frances Y. Kuo, and Ian H. Sloan. &quot;High-dimensional 
<a name="l168"><span class="ln">168  </span></a>   integration: the quasi-Monte Carlo way.&quot; Acta Numerica no. 22: 133, 2013. 
<a name="l169"><span class="ln">169  </span></a>.. [4] Aho, A. V., C. Aistleitner, T. Anderson, K. Appel, V. Arnol'd, N. 
<a name="l170"><span class="ln">170  </span></a>   Aronszajn, D. Asotsky et al. &quot;W. Chen et al.(eds.), &quot;A Panorama of 
<a name="l171"><span class="ln">171  </span></a>   Discrepancy Theory&quot;, Sringer International Publishing, 
<a name="l172"><span class="ln">172  </span></a>   Switzerland: 679, 2014. 
<a name="l173"><span class="ln">173  </span></a>.. [5] Hickernell, Fred J. &quot;Koksma-Hlawka Inequality.&quot; Wiley StatsRef: 
<a name="l174"><span class="ln">174  </span></a>   Statistics Reference Online, 2014. 
<a name="l175"><span class="ln">175  </span></a>.. [6] Owen, Art B. &quot;On dropping the first Sobol' point.&quot; :arxiv:`2008.08051`, 
<a name="l176"><span class="ln">176  </span></a>   2020. 
<a name="l177"><span class="ln">177  </span></a>.. [7] L'Ecuyer, Pierre, and Christiane Lemieux. &quot;Recent advances in randomized 
<a name="l178"><span class="ln">178  </span></a>   quasi-Monte Carlo methods.&quot; In Modeling uncertainty, pp. 419-474. Springer, 
<a name="l179"><span class="ln">179  </span></a>   New York, NY, 2002. 
<a name="l180"><span class="ln">180  </span></a>.. [8] DiCiccio, Thomas J., and Bradley Efron. &quot;Bootstrap confidence 
<a name="l181"><span class="ln">181  </span></a>   intervals.&quot; Statistical science: 189-212, 1996. 
<a name="l182"><span class="ln">182  </span></a>.. [9] Dimov, Ivan T. &quot;Monte Carlo methods for applied scientists.&quot; World 
<a name="l183"><span class="ln">183  </span></a>   Scientific, 2008. 
<a name="l184"><span class="ln">184  </span></a>.. [10] Caflisch, Russel E., William J. Morokoff, and Art B. Owen. &quot;Valuation 
<a name="l185"><span class="ln">185  </span></a>   of mortgage backed securities using Brownian bridges to reduce effective 
<a name="l186"><span class="ln">186  </span></a>   dimension.&quot; Journal of Computational Finance: no. 1 27-46, 1997. 
<a name="l187"><span class="ln">187  </span></a>.. [11] Sloan, Ian H., and Henryk Wozniakowski. &quot;When are quasi-Monte Carlo 
<a name="l188"><span class="ln">188  </span></a>   algorithms efficient for high dimensional integrals?.&quot; Journal of Complexity 
<a name="l189"><span class="ln">189  </span></a>   14, no. 1 (1998): 1-33. 
<a name="l190"><span class="ln">190  </span></a>.. [12] Owen, Art B., and Daniel Rudolf, &quot;A strong law of large numbers for 
<a name="l191"><span class="ln">191  </span></a>   scrambled net integration.&quot; SIAM Review, to appear. 
<a name="l192"><span class="ln">192  </span></a>.. [13] Loh, Wei-Liem. &quot;On the asymptotic distribution of scrambled net 
<a name="l193"><span class="ln">193  </span></a>   quadrature.&quot; The Annals of Statistics 31, no. 4: 1282-1324, 2003. 
<a name="l194"><span class="ln">194  </span></a>.. [14] Sloan, Ian H. and S. Joe. &quot;Lattice methods for multiple integration.&quot; 
<a name="l195"><span class="ln">195  </span></a>   Oxford University Press, 1994. 
<a name="l196"><span class="ln">196  </span></a>.. [15] Dick, Josef, and Friedrich Pillichshammer. &quot;Digital nets and sequences: 
<a name="l197"><span class="ln">197  </span></a>   discrepancy theory and quasi-Monte Carlo integration.&quot; Cambridge University 
<a name="l198"><span class="ln">198  </span></a>   Press, 2010. 
<a name="l199"><span class="ln">199  </span></a>.. [16] Dick, Josef, F. Kuo, Friedrich Pillichshammer, and I. Sloan. 
<a name="l200"><span class="ln">200  </span></a>   &quot;Construction algorithms for polynomial lattice rules for multivariate 
<a name="l201"><span class="ln">201  </span></a>   integration.&quot; Mathematics of computation 74, no. 252: 1895-1921, 2005. 
<a name="l202"><span class="ln">202  </span></a>.. [17] Sobol', Il'ya Meerovich. &quot;On the distribution of points in a cube and 
<a name="l203"><span class="ln">203  </span></a>   the approximate evaluation of integrals.&quot; Zhurnal Vychislitel'noi Matematiki 
<a name="l204"><span class="ln">204  </span></a>   i Matematicheskoi Fiziki 7, no. 4: 784-802, 1967. 
<a name="l205"><span class="ln">205  </span></a>.. [18] Halton, John H. &quot;On the efficiency of certain quasi-random sequences of 
<a name="l206"><span class="ln">206  </span></a>   points in evaluating multi-dimensional integrals.&quot; Numerische Mathematik 2, 
<a name="l207"><span class="ln">207  </span></a>   no. 1: 84-90, 1960. 
<a name="l208"><span class="ln">208  </span></a>.. [19] Faure, Henri. &quot;Discrepance de suites associees a un systeme de 
<a name="l209"><span class="ln">209  </span></a>   numeration (en dimension s).&quot; Acta arithmetica 41, no. 4: 337-351, 1982. 
<a name="l210"><span class="ln">210  </span></a>.. [20] Niederreiter, Harold, and Chaoping Xing. &quot;Low-discrepancy sequences and 
<a name="l211"><span class="ln">211  </span></a>   global function fields with many rational places.&quot; Finite Fields and their 
<a name="l212"><span class="ln">212  </span></a>   applications 2, no. 3: 241-273, 1996. 
<a name="l213"><span class="ln">213  </span></a>.. [21] Hong, Hee Sun, and Fred J. Hickernell. &quot;Algorithm 823: Implementing 
<a name="l214"><span class="ln">214  </span></a>   scrambled digital sequences.&quot; ACM Transactions on Mathematical Software 
<a name="l215"><span class="ln">215  </span></a>   (TOMS) 29, no. 2: 95-109, 2003. 
<a name="l216"><span class="ln">216  </span></a>.. [22] Dick, Josef. &quot;Higher order scrambled digital nets achieve the optimal 
<a name="l217"><span class="ln">217  </span></a>   rate of the root mean square error for smooth integrands.&quot; The Annals of 
<a name="l218"><span class="ln">218  </span></a>   Statistics 39, no. 3: 1372-1398, 2011. 
<a name="l219"><span class="ln">219  </span></a>.. [23] Niederreiter, Harald. &quot;Multidimensional numerical integration using 
<a name="l220"><span class="ln">220  </span></a>   pseudorandom numbers.&quot; In Stochastic Programming 84 Part I, pp. 17-38. 
<a name="l221"><span class="ln">221  </span></a>   Springer, Berlin, Heidelberg, 1986. 
<a name="l222"><span class="ln">222  </span></a>.. [24] Hickernell, Fred J. &quot;Obtaining O (N-2+e) Convergence for Lattice 
<a name="l223"><span class="ln">223  </span></a>   Quadrature Rules.&quot; In Monte Carlo and Quasi-Monte Carlo Methods 2000, 
<a name="l224"><span class="ln">224  </span></a>   pp. 274-289. Springer, Berlin, Heidelberg, 2002. 
<a name="l225"><span class="ln">225  </span></a>.. [25] Owen, Art B., and Seth D. Tribble. &quot;A quasi-Monte Carlo Metropolis 
<a name="l226"><span class="ln">226  </span></a>   algorithm.&quot; Proceedings of the National Academy of Sciences 102, 
<a name="l227"><span class="ln">227  </span></a>   no. 25: 8844-8849, 2005. 
<a name="l228"><span class="ln">228  </span></a>.. [26] Chen, Su. &quot;Consistency and convergence rate of Markov chain quasi Monte 
<a name="l229"><span class="ln">229  </span></a>   Carlo with examples.&quot; PhD diss., Stanford University, 2011. 
<a name="l230"><span class="ln">230  </span></a>.. [27] Joe, Stephen, and Frances Y. Kuo. &quot;Constructing Sobol sequences with 
<a name="l231"><span class="ln">231  </span></a>   better two-dimensional projections.&quot; SIAM Journal on Scientific Computing 
<a name="l232"><span class="ln">232  </span></a>   30, no. 5: 2635-2654, 2008. 
<a name="l233"><span class="ln">233  </span></a> 
<a name="l234"><span class="ln">234  </span></a>&quot;&quot;&quot;</span>
<a name="l235"><span class="ln">235  </span></a><span class="s2">from </span><span class="s3">.</span><span class="s1">_qmc </span><span class="s2">import </span><span class="s3">*  </span><span class="s4"># noqa: F403</span>
<a name="l236"><span class="ln">236  </span></a><span class="s2">from </span><span class="s3">.</span><span class="s1">_qmc </span><span class="s2">import </span><span class="s1">__all__  </span><span class="s4"># noqa: F401</span>
<a name="l237"><span class="ln">237  </span></a></pre>
</body>
</html>